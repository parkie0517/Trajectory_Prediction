{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8yHFGvoG8qx"
   },
   "source": [
    "Describe your code using comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDKuYqrI5CPt"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZMGiWXRV4wvi"
   },
   "outputs": [],
   "source": [
    "root = '/home/vilab/ssd1tb/hj_ME455/Trajectory_Prediction'\n",
    "import sys\n",
    "sys.path.append(root)\n",
    "\n",
    "import argparse\n",
    "import gc\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from trajectories import data_loader\n",
    "from utils import relative_to_abs, get_dset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "k-h-wC5k4wvk"
   },
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "cfg = edict()\n",
    "\n",
    "cfg.dataset_name = 'zara1'\n",
    "cfg.delim = '\\t'\n",
    "cfg.loader_num_workers = 4\n",
    "\n",
    "cfg.obs_len = 8\n",
    "cfg.pred_len = 8\n",
    "cfg.skip = 1\n",
    "\n",
    "cfg.batch_size = 16\n",
    "cfg.num_epochs = 200\n",
    "cfg.learning_rate = 5e-4\n",
    "\n",
    "cfg.embedding_dim = 64\n",
    "cfg.h_dim = 64\n",
    "cfg.num_layers = 1\n",
    "cfg.mlp_dim = 1024\n",
    "cfg.dropout = 0\n",
    "\n",
    "cfg.best_k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_dOcb3lZ4wvl"
   },
   "outputs": [],
   "source": [
    "train_path = get_dset_path(cfg.dataset_name, 'train')\n",
    "val_path = get_dset_path(cfg.dataset_name, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uY-tO6gg4wvl"
   },
   "outputs": [],
   "source": [
    "train_dset, train_loader = data_loader(cfg, train_path)\n",
    "_, val_loader = data_loader(cfg, val_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for trajectory prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, embedding_dim=64, h_dim=64, num_layers=1, type='lstm'):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.h_dim = h_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.encoder = nn.LSTM(embedding_dim, h_dim, num_layers)\n",
    "\n",
    "        self.en_mlp = nn.Linear(2, embedding_dim)\n",
    "        self.de_mlp = nn.Linear(h_dim, 2)\n",
    "\n",
    "    def make_state(self, batch):\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, batch, self.h_dim).cuda(),\n",
    "            torch.zeros(self.num_layers, batch, self.h_dim).cuda()\n",
    "        )\n",
    "\n",
    "    def forward(self, obs_traj):\n",
    "        batch = obs_traj.size(1)\n",
    "        encoder_input =  self.en_mlp(obs_traj)\n",
    "        encoder_input = encoder_input.view(-1, batch, self.embedding_dim)\n",
    "        \n",
    "        state = self.make_state(batch)\n",
    "        encoder_output, state = self.encoder(encoder_input, state)\n",
    "        \n",
    "        pred = self.de_mlp(encoder_output.reshape(-1, self.h_dim))\n",
    "        pred = pred.view(-1, batch, 2)\n",
    "        \n",
    "        return encoder_output, state\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, seq_len, embedding_dim=64, h_dim=128, num_layers=1, type='lstm'):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.h_dim = h_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.model_type = type\n",
    "        \n",
    "        self.decoder = nn.LSTM(embedding_dim, h_dim, num_layers)\n",
    "\n",
    "        self.en_mlp = nn.Linear(2, embedding_dim)\n",
    "        self.de_mlp = nn.Linear(h_dim, 2)\n",
    "\n",
    "    def forward(self, last_pos, last_pos_rel, state):\n",
    "        batch = last_pos_rel.size(0)\n",
    "        pred_traj_fake_rel = []\n",
    "        decoder_input = self.en_mlp(last_pos_rel)\n",
    "        decoder_input = decoder_input.view(1, batch, self.embedding_dim)\n",
    "\n",
    "        for _ in range(self.seq_len):\n",
    "            decoder_output, state = self.decoder(decoder_input, state)\n",
    "            pos_rel = self.de_mlp(decoder_output.view(-1, self.h_dim))\n",
    "            curr_pos = last_pos + pos_rel\n",
    "\n",
    "            decoder_input = self.en_mlp(pos_rel).view(1, batch, self.embedding_dim)\n",
    "            pred_traj_fake_rel.append(pos_rel)\n",
    "            last_pos = curr_pos\n",
    "\n",
    "        pred_traj_fake_rel = torch.stack(pred_traj_fake_rel, dim=0)\n",
    "        return pred_traj_fake_rel, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, cfg, type):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.encoder = Encoder(embedding_dim=cfg.embedding_dim, h_dim=cfg.h_dim, num_layers=cfg.num_layers, type='lstm')\n",
    "        self.decoder = Decoder(seq_len=cfg.pred_len-1, embedding_dim=cfg.embedding_dim, h_dim=cfg.h_dim, num_layers=cfg.num_layers, type='lstm')\n",
    "\n",
    "    \n",
    "    def forward(self, obs_traj, obs_traj_rel):\n",
    "        last_pos = obs_traj[-1]\n",
    "        last_pos_rel = obs_traj_rel[-1]\n",
    "\n",
    "        encoder_output, state = self.encoder(obs_traj)\n",
    "        pred_traj_fake_rel, state = self.decoder(last_pos, last_pos_rel, state)\n",
    "        \n",
    "        pred_traj_rel = torch.unsqueeze(obs_traj_rel[-1], 0) \n",
    "        pred_traj_fake_rel = torch.cat([pred_traj_rel, pred_traj_fake_rel], dim=0)\n",
    "        \n",
    "        return pred_traj_fake_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ade(fake, gt):\n",
    "    ade = gt.permute(1, 0, 2) - fake.permute(1, 0, 2)\n",
    "    ade = ade**2\n",
    "    ade = torch.sqrt(ade.sum(dim=2)).sum(dim=1)\n",
    "    return torch.sum(ade)\n",
    "\n",
    "\n",
    "def cal_fde(fake, gt):\n",
    "    fde = gt - fake\n",
    "    fde = fde**2\n",
    "    fde = torch.sqrt(fde.sum(dim=1))\n",
    "    return torch.sum(fde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred_traj_fake_rel, pred_traj_gt_rel, loss_mask):\n",
    "    fake = loss_mask.unsqueeze(2)*pred_traj_fake_rel.permute(1, 0, 2)\n",
    "    gt =  loss_mask.unsqueeze(2)*pred_traj_gt_rel.permute(1, 0, 2)\n",
    "    \n",
    "    l2_loss = torch.nn.MSELoss()\n",
    "    \n",
    "    \n",
    "    return l2_loss(fake, gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the train lodaer and validation loader for training the model.\n",
    "\n",
    "While validation we will calculate ade and fde using pre-defined functions above.\n",
    "To do this, we will use \"relative_to_abs\" function to change realtive position to absolute position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model_lstm = MyModel(cfg, type='lstm')\n",
    "\n",
    "model_lstm.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "\n",
    "os.makedirs(os.path.join(root, 'ckpt_lstm'), exist_ok=True)\n",
    "save_path = os.path.join(root, 'ckpt_lstm/ckpt_lstm_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cHnh_C-34wvm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [00:44<14:00,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, train_loss: 0.007599214105673972, val_ade: 0.3013277053833008, val_fde: 0.3802464008331299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 20/200 [01:29<13:27,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, train_loss: 0.007311215764738223, val_ade: 0.29600876569747925, val_fde: 0.38363611698150635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 30/200 [02:14<12:36,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30, train_loss: 0.007052944387590303, val_ade: 0.290357381105423, val_fde: 0.3792865574359894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 40/200 [02:59<12:00,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40, train_loss: 0.006971574380178247, val_ade: 0.3048304617404938, val_fde: 0.3884195387363434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 50/200 [03:43<11:18,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50, train_loss: 0.006827848668612495, val_ade: 0.2833017110824585, val_fde: 0.3732741177082062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 60/200 [04:29<10:43,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 60, train_loss: 0.006664948988452378, val_ade: 0.2913244962692261, val_fde: 0.3790520429611206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 70/200 [05:15<09:54,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 70, train_loss: 0.006510966578663806, val_ade: 0.2861684560775757, val_fde: 0.3795682489871979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 80/200 [05:59<09:00,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80, train_loss: 0.006375434652753395, val_ade: 0.29341068863868713, val_fde: 0.38223540782928467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 90/200 [06:44<08:15,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 90, train_loss: 0.006287088625948634, val_ade: 0.30363062024116516, val_fde: 0.3876529932022095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 100/200 [07:29<07:30,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, train_loss: 0.006137465301779865, val_ade: 0.3019447326660156, val_fde: 0.3845038115978241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 110/200 [08:14<06:43,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 110, train_loss: 0.005936741359124082, val_ade: 0.2914031744003296, val_fde: 0.3811647593975067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 120/200 [08:59<06:07,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 120, train_loss: 0.005915057374241613, val_ade: 0.29874396324157715, val_fde: 0.38651323318481445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 130/200 [09:44<05:08,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 130, train_loss: 0.005823771969398715, val_ade: 0.2954752743244171, val_fde: 0.3857441544532776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 140/200 [10:28<04:29,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 140, train_loss: 0.005733573716960451, val_ade: 0.2985149025917053, val_fde: 0.3891157805919647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 150/200 [11:13<03:45,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 150, train_loss: 0.005609668992437731, val_ade: 0.29998156428337097, val_fde: 0.3895070552825928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 160/200 [11:59<03:02,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 160, train_loss: 0.005534043140191386, val_ade: 0.30115070939064026, val_fde: 0.3919069468975067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 170/200 [12:44<02:14,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 170, train_loss: 0.0053989481905574455, val_ade: 0.29978519678115845, val_fde: 0.3932998478412628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 180/200 [13:29<01:29,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 180, train_loss: 0.00537403701865197, val_ade: 0.31395450234413147, val_fde: 0.39907920360565186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 190/200 [14:14<00:45,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 190, train_loss: 0.005283455152240907, val_ade: 0.3062886595726013, val_fde: 0.3977021276950836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [14:59<00:00,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 200, train_loss: 0.005184947397317407, val_ade: 0.3047943413257599, val_fde: 0.3929680287837982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(cfg.num_epochs)):\n",
    "    train_loss_value = 0\n",
    "    total_traj = 0\n",
    "    ade, fde = 0, 0\n",
    "    \n",
    "    model_lstm.train()\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = [tensor.cuda() for tensor in batch]\n",
    "        (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, _, loss_mask, seq_start_end) = batch\n",
    "        loss_mask = loss_mask[:, cfg.obs_len:]\n",
    "        \n",
    "        #############################\n",
    "        ### fill the train loader ###\n",
    "        #############################\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_traj_fake_rel = model_lstm(obs_traj, obs_traj_rel)  # Forward pass\n",
    "        loss = loss_fn(pred_traj_fake_rel, pred_traj_gt_rel, loss_mask)  # Calculate loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        train_loss_value += loss.item()\n",
    "        \n",
    "    train_loss_value /= len(train_loader)\n",
    "        \n",
    "    model_lstm.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = [tensor.cuda() for tensor in batch]\n",
    "            (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, _, loss_mask, seq_start_end) = batch\n",
    "            loss_mask = loss_mask[:, cfg.obs_len:]\n",
    "            \n",
    "            ########################################################################\n",
    "            #### fill the validation loader ########################################\n",
    "            # hint: use \"relative_to_abs\" function to validate your result #########\n",
    "            ########################################################################\n",
    "            pred_traj_fake_rel = model_lstm(obs_traj, obs_traj_rel)\n",
    "            pred_traj_fake = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])\n",
    "\n",
    "            ade += cal_ade(pred_traj_fake, pred_traj_gt)\n",
    "            fde += cal_fde(pred_traj_fake, pred_traj_gt)\n",
    "\n",
    "            total_traj += pred_traj_gt.size(1)\n",
    "        \n",
    "\n",
    "        \n",
    "    ade = ade / (total_traj * cfg.pred_len)  # Normalize ADE by total trajectories and sequence length\n",
    "    fde = fde / total_traj  # Normalize FDE by total trajectories\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': i,\n",
    "        'model_state_dict': model_lstm.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, f'{save_path}{i:03d}.pt')\n",
    "\n",
    "    if (i + 1) % 10 == 0:\n",
    "        e = i + 1\n",
    "        print(f'epoch: {e}, train_loss: {train_loss_value}, val_ade: {ade}, val_fde: {fde}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the prediction results with your trajectory prediction model and its checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter ffmpeg unavailable; using Pillow instead.\n"
     ]
    }
   ],
   "source": [
    "model_lstm = MyModel(cfg, type='lstm')  # Ensure cfg is properly defined\n",
    "checkpoint = torch.load(os.path.join(root, 'ckpt_lstm/ckpt_lstm_199.pt'))  # Load the last checkpoint\n",
    "model_lstm.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_lstm.eval().to(device)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import random\n",
    "\n",
    "r = len(val_loader)\n",
    "rr = random.randrange(0,r)\n",
    "\n",
    "batch_cnt = 0   \n",
    "for batch in val_loader:\n",
    "    if batch_cnt == rr:\n",
    "        batch = [tensor.cuda() for tensor in batch]\n",
    "        (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, _, loss_mask, seq_start_end) = batch\n",
    "        \n",
    "        loss_mask = loss_mask[:, cfg.obs_len:]\n",
    "        \n",
    "        ################## fill the validation loader ####################\n",
    "        ## hint: use \"relative_to_abs\" function to validate your result ##\n",
    "        pred_traj_fake_rel = model_lstm(obs_traj, obs_traj_rel)\n",
    "        pred_traj_fake_lstm = relative_to_abs(pred_traj_fake_rel, obs_traj[-1])\n",
    "        ##################################################################\n",
    "        \n",
    "        for i in range(seq_start_end.size(0)):\n",
    "            startpoint = seq_start_end[i][0]\n",
    "            endpoint = seq_start_end[i][1]\n",
    "            ped_num = endpoint - startpoint\n",
    "            if endpoint - startpoint < 3:\n",
    "                break\n",
    "            \n",
    "        obs_x, obs_y = obs_traj[:,startpoint:endpoint,0].detach().cpu().numpy(), obs_traj[:,startpoint:endpoint,1].detach().cpu().numpy()\n",
    "        gt_x, gt_y = pred_traj_gt[:,startpoint:endpoint,0].detach().cpu().numpy(), pred_traj_gt[:,startpoint:endpoint,1].detach().cpu().numpy()\n",
    "        pred_x_lstm, pred_y_lstm = pred_traj_fake_lstm[:,startpoint:endpoint,0].detach().cpu().numpy(), pred_traj_fake_lstm[:,startpoint:endpoint,1].detach().cpu().numpy()\n",
    "\n",
    "        fig = plt.figure()\n",
    "        fig.set_size_inches(7, 7)\n",
    "        \n",
    "        plt.xlim([-15, 15])      \n",
    "        plt.ylim([-15, 15])\n",
    "        \n",
    "        list_x, list_y = [], []\n",
    "\n",
    "        for i in range(8):\n",
    "            for j in range(ped_num):\n",
    "                list_x.append(obs_x[i][j])\n",
    "                list_y.append(obs_y[i][j])\n",
    "\n",
    "        line, = plt.plot(list_x, list_y, 'b.', label='obersed')\n",
    "        \n",
    "        list_x_fake, list_y_fake = [], []\n",
    "        line_fake, = plt.plot([], [], 'ro', label='predicted')\n",
    "        \n",
    "        list_x_gt, list_y_gt = [], []\n",
    "        line_gt, = plt.plot([], [], 'g*', label = 'gt')\n",
    "        \n",
    "        plt.title('LSTM Traj')\n",
    "        plt.legend()\n",
    "        \n",
    "        def update(i):\n",
    "            for j in range(ped_num):\n",
    "                list_x_fake.append(pred_x_lstm[i][j])\n",
    "                list_y_fake.append(pred_y_lstm[i][j])\n",
    "                list_x_gt.append(gt_x[i][j])\n",
    "                list_y_gt.append(gt_y[i][j])\n",
    "            line_fake.set_data(list_x_fake, list_y_fake)\n",
    "            line_gt.set_data(list_x_gt, list_y_gt)\n",
    "            return line_fake, line_gt,\n",
    "        \n",
    "\n",
    "        anim = FuncAnimation(fig, update, frames=[0,1,2,3,4,5,6,7],interval=1000)\n",
    "        anim.save(os.path.join(root, \"traj_lstm.gif\"), fps=1)\n",
    "        plt.close()\n",
    "\n",
    "        break\n",
    "    batch_cnt += 1\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "hj_CRN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
