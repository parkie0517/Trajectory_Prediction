{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8yHFGvoG8qx"
   },
   "source": [
    "Describe your code using comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDKuYqrI5CPt"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMGiWXRV4wvi"
   },
   "outputs": [],
   "source": [
    "root = '/home/vilab/ssd1tb/hj_ME455/Trajectory_Prediction'\n",
    "import sys\n",
    "sys.path.append(root)\n",
    "\n",
    "import argparse\n",
    "import gc\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from trajectories import data_loader\n",
    "from utils import relative_to_abs, get_dset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-h-wC5k4wvk"
   },
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "cfg = edict()\n",
    "\n",
    "cfg.dataset_name = 'zara1'\n",
    "cfg.delim = '\\t'\n",
    "cfg.loader_num_workers = 4\n",
    "\n",
    "cfg.obs_len = 8\n",
    "cfg.pred_len = 8\n",
    "cfg.skip = 1\n",
    "\n",
    "cfg.batch_size = 16\n",
    "cfg.num_epochs = 200\n",
    "cfg.learning_rate = 5e-4\n",
    "\n",
    "cfg.embedding_dim = 64\n",
    "cfg.h_dim = 64\n",
    "cfg.num_layers = 1\n",
    "cfg.mlp_dim = 1024\n",
    "cfg.dropout = 0\n",
    "\n",
    "cfg.best_k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dOcb3lZ4wvl"
   },
   "outputs": [],
   "source": [
    "train_path = get_dset_path(cfg.dataset_name, 'train')\n",
    "val_path = get_dset_path(cfg.dataset_name, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uY-tO6gg4wvl"
   },
   "outputs": [],
   "source": [
    "train_dset, train_loader = data_loader(cfg, train_path)\n",
    "_, val_loader = data_loader(cfg, val_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for trajectory prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, embedding_dim=64, h_dim=64, num_layers=1, type='lstm'):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.h_dim = h_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.encoder = nn.LSTM(???, ???, ???)\n",
    "\n",
    "        self.en_mlp = nn.Linear(???, ???)\n",
    "        self.de_mlp = nn.Linear(???, ???)\n",
    "\n",
    "    def make_state(self, batch):\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, batch, self.h_dim).cuda(),\n",
    "            torch.zeros(self.num_layers, batch, self.h_dim).cuda()\n",
    "        )\n",
    "\n",
    "    def forward(self, obs_traj):\n",
    "        batch = obs_traj.size(1)\n",
    "        encoder_input = self.en_mlp(???)\n",
    "        encoder_input = encoder_input.view(???, ???, ???)\n",
    "        \n",
    "        state = self.make_state(batch)\n",
    "        encoder_output, state = self.encoder(???, ???)\n",
    "        \n",
    "        pred = self.de_mlp(encoder_output.reshape(???, ???))\n",
    "        pred = pred.view(-1, batch, 2)\n",
    "        \n",
    "        return ???, ???\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, seq_len, embedding_dim=64, h_dim=128, num_layers=1, type='lstm'):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.h_dim = h_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.model_type = type\n",
    "        \n",
    "        self.decoder = nn.LSTM(???, ???, ???)\n",
    "\n",
    "        self.en_mlp = nn.Linear(???, ???)\n",
    "        self.de_mlp = nn.Linear(???, ???)\n",
    "\n",
    "    def forward(self, last_pos, last_pos_rel, state):\n",
    "        batch = last_pos_rel.size(0)\n",
    "        pred_traj_fake_rel = []\n",
    "        decoder_input = self.en_mlp(???)\n",
    "        decoder_input = decoder_input.view(???, ???, ???)\n",
    "\n",
    "        for _ in range(???):\n",
    "            decoder_output, state = self.decoder(???, ???)\n",
    "            pos_rel = self.de_mlp(???)\n",
    "            curr_pos = ??? + ???\n",
    "\n",
    "            ??? = self.en_mlp(pos_rel).view(???, ???, ???)\n",
    "            pred_traj_fake_rel.append(???)\n",
    "            last_pos = ???\n",
    "\n",
    "        pred_traj_fake_rel = torch.stack(pred_traj_fake_rel, dim=0)\n",
    "        return ???, ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, cfg, type):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.encoder = Encoder(embedding_dim=cfg.embedding_dim, h_dim=cfg.h_dim, num_layers=cfg.num_layers, type='lstm')\n",
    "        self.decoder = Decoder(seq_len=cfg.pred_len-1, embedding_dim=cfg.embedding_dim, h_dim=cfg.h_dim, num_layers=cfg.num_layers, type='lstm')\n",
    "\n",
    "    \n",
    "    def forward(self, obs_traj, obs_traj_rel):\n",
    "        last_pos = ???\n",
    "        \n",
    "        ???, ??? = self.encoder(???)\n",
    "        ???, ??? = self.decoder(???, ???, ???)\n",
    "        \n",
    "        pred_traj_rel = torch.unsqueeze(pred_traj_rel,0) \n",
    "        pred_traj_fake_rel = torch.cat([pred_traj_rel, pred_traj_fake_rel], dim=0)\n",
    "        \n",
    "        return pred_traj_fake_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ade(fake, gt):\n",
    "    ade = gt.permute(1, 0, 2) - fake.permute(1, 0, 2)\n",
    "    ade = ade**2\n",
    "    ade = torch.sqrt(ade.sum(dim=2)).sum(dim=1)\n",
    "    return torch.sum(ade)\n",
    "\n",
    "\n",
    "def cal_fde(fake, gt):\n",
    "    fde = gt - fake\n",
    "    fde = fde**2\n",
    "    fde = torch.sqrt(fde.sum(dim=1))\n",
    "    return torch.sum(fde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred_traj_fake_rel, pred_traj_gt_rel, loss_mask):\n",
    "    fake = loss_mask.unsqueeze(2)*pred_traj_fake_rel.permute(1, 0, 2)\n",
    "    gt =  loss_mask.unsqueeze(2)*pred_traj_gt_rel.permute(1, 0, 2)\n",
    "    \n",
    "    l2_loss = torch.nn.MSELoss()\n",
    "    \n",
    "    \n",
    "    return l2_loss(fake, gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the train lodaer and validation loader for training the model.\n",
    "\n",
    "While validation we will calculate ade and fde using pre-defined functions above.\n",
    "To do this, we will use \"relative_to_abs\" function to change realtive position to absolute position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model_lstm = ???\n",
    "\n",
    "model_lstm.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "\n",
    "os.makedirs(os.path.join(root, 'ckpt_lstm'), exist_ok=True)\n",
    "save_path = os.path.join(root, 'ckpt_lstm/ckpt_lstm_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHnh_C-34wvm"
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(cfg.num_epochs)):\n",
    "    for batch in train_loader:\n",
    "        batch = [tensor.cuda() for tensor in batch]\n",
    "        (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, _, loss_mask, seq_start_end) = batch\n",
    "        loss_mask = loss_mask[:, cfg.obs_len:]\n",
    "        ### fill the train loader ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##################################\n",
    "\n",
    "    for batch in val_loader:\n",
    "        batch = [tensor.cuda() for tensor in batch]\n",
    "        (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, _, loss_mask, seq_start_end) = batch\n",
    "        loss_mask = loss_mask[:, cfg.obs_len:]\n",
    "        ### fill the validation loader ### # hint: use \"relative_to_abs\" function to validate your result\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##################################\n",
    "        \n",
    "    ade = sum(ade) / (total_traj * cfg.pred_len)\n",
    "    fde = sum(fde) / (total_traj)\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': i,\n",
    "        'model_state_dict': model_lstm.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, f'{save_path}{i:03d}.pt')\n",
    "\n",
    "    if (i + 1) % 10 == 0:\n",
    "        e = i + 1\n",
    "        print(f'epoch: {e}, train_loss: {train_loss_value}, val_ade: {ade}, val_fde: {fde}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the prediction results with your trajectory prediction model and its checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = ?????\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import random\n",
    "\n",
    "r = len(val_loader)\n",
    "        \n",
    "rr = random.randrange(0,r)\n",
    "\n",
    "batch_cnt = 0   \n",
    "for batch in val_loader:\n",
    "    if batch_cnt == rr:\n",
    "        batch = [tensor.cuda() for tensor in batch]\n",
    "        (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, _, loss_mask, seq_start_end) = batch\n",
    "        \n",
    "        loss_mask = loss_mask[:, cfg.obs_len:]\n",
    "        \n",
    "        ### fill the validation loader ### # hint: use \"relative_to_abs\" function to validate your result\n",
    "        \n",
    "        \n",
    "        pred_traj_fake_lstm = ???\n",
    "        ##################################\n",
    "        \n",
    "        for i in range(seq_start_end.size(0)):\n",
    "            startpoint = seq_start_end[i][0]\n",
    "            endpoint = seq_start_end[i][1]\n",
    "            ped_num = endpoint - startpoint\n",
    "            if endpoint - startpoint < 3:\n",
    "                break\n",
    "            \n",
    "        obs_x, obs_y = obs_traj[:,startpoint:endpoint,0].detach().cpu().numpy(), obs_traj[:,startpoint:endpoint,1].detach().cpu().numpy()\n",
    "        gt_x, gt_y = pred_traj_gt[:,startpoint:endpoint,0].detach().cpu().numpy(), pred_traj_gt[:,startpoint:endpoint,1].detach().cpu().numpy()\n",
    "        pred_x_lstm, pred_y_lstm = pred_traj_fake_lstm[:,startpoint:endpoint,0].detach().cpu().numpy(), pred_traj_fake_lstm[:,startpoint:endpoint,1].detach().cpu().numpy()\n",
    "\n",
    "        fig = plt.figure()\n",
    "        fig.set_size_inches(7, 7)\n",
    "        \n",
    "        plt.xlim([-15, 15])      \n",
    "        plt.ylim([-15, 15])\n",
    "        \n",
    "        list_x, list_y = [], []\n",
    "\n",
    "        for i in range(8):\n",
    "            for j in range(ped_num):\n",
    "                list_x.append(obs_x[i][j])\n",
    "                list_y.append(obs_y[i][j])\n",
    "\n",
    "        line, = plt.plot(list_x, list_y, 'b.', label='obersed')\n",
    "        \n",
    "        list_x_fake, list_y_fake = [], []\n",
    "        line_fake, = plt.plot([], [], 'ro', label='predicted')\n",
    "        \n",
    "        list_x_gt, list_y_gt = [], []\n",
    "        line_gt, = plt.plot([], [], 'g*', label = 'gt')\n",
    "        \n",
    "        plt.title('LSTM Traj')\n",
    "        plt.legend()\n",
    "        \n",
    "        def update(i):\n",
    "            for j in range(ped_num):\n",
    "                list_x_fake.append(pred_x_lstm[i][j])\n",
    "                list_y_fake.append(pred_y_lstm[i][j])\n",
    "                list_x_gt.append(gt_x[i][j])\n",
    "                list_y_gt.append(gt_y[i][j])\n",
    "            line_fake.set_data(list_x_fake, list_y_fake)\n",
    "            line_gt.set_data(list_x_gt, list_y_gt)\n",
    "            return line_fake, line_gt,\n",
    "        \n",
    "\n",
    "        anim = FuncAnimation(fig, update, frames=[0,1,2,3,4,5,6,7],interval=1000)\n",
    "        anim.save(os.path.join(root, \"traj_lstm.gif\"), fps=1)\n",
    "        plt.close()\n",
    "\n",
    "        break\n",
    "    batch_cnt += 1\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "hj_CRN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
